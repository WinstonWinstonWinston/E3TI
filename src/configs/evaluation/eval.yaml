evaluate:
  metrics: ["accuracy", "confusion_matrix"]
  save_outputs: true
