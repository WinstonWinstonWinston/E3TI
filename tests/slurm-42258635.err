/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
wandb: Currently logged in as: winsaton (winsaton-univeristy-of-minnesota) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 6zvl6wpu
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /users/1/sull1276/E3-Tensor-Interpolants/tests/logs/wandb/wandb/run-20251015_165007-6zvl6wpu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run E3TensorInterpolants
wandb: ‚≠êÔ∏è View project at https://wandb.ai/winsaton-univeristy-of-minnesota/e3ti
wandb: üöÄ View run at https://wandb.ai/winsaton-univeristy-of-minnesota/e3ti/runs/6zvl6wpu
wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /users/1/sull1276/E3-Tensor-Interpolants/tests/logs/hydra/ckpt exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name     | Type                | Params | Mode 
---------------------------------------------------------
0 | embedder | EquilibriumEmbedder | 11.6 K | train
1 | model    | MultiSE3Transformer | 1.2 M  | train
---------------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.649     Total estimated model params size (MB)
123       Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1408. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val/loss_velocity', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/users/1/sull1276/micromamba/envs/e3ti/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val/loss_denoiser', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
